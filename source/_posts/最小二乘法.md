---
title: 最小二乘法
tags: 机器学习
swiper_index: 12
categories: 机器学习
description: 最小二乘法原理及讲解
cover: 'https://img.tucang.cc/api/image/show/454f992a776c6f7cbfae9db6d113749f'
abbrlink: 6b330b18
date: 2024-06-05 23:44:08
---



## 1. 最小二乘法目标函数

目标函数是观测值和理论值之间误差的平方和。假设有 $m$ 个数据点，每个数据点 $(x_i, y_i)$ 有一个观测值 $y_i$ 和一个预测值 $h_\theta(x_i)$，目标函数可以表示为：

 $\text{目标函数} = \sum_{i=1}^m (y_i - h_\theta(x_i))^2$ 

## 2. 线性回归模型

线性回归模型表示为：
 $$h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 + \ldots + \theta_nx^n$$ 

对于简单的线性回归，即只有一个特征 $x$，模型简化为：
 $$h_\theta(x) = \theta_0 + \theta_1x$$ 

使用向量和矩阵表示，更加简洁和通用。假设有 $n$ 个特征，那么输入矩阵 $\mathbf{X}$ 为：
 $$\mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1n} \\ 1 & x_{21} & x_{22} & \cdots & x_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{m1} & x_{m2} & \cdots & x_{mn} \end{bmatrix}$$ 

参数向量 $\theta$ 为：
 $$\theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{bmatrix}$$ 

预测值可以表示为：
 $$h_\theta(\mathbf{x}) = \mathbf{X}\theta$$ 

## 3. 损失函数

损失函数 $J(\theta)$ 定义为：
 $$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x_i) - y_i)^2$$ 

将其表示为矩阵形式：
 $$J(\theta) = \frac{1}{2m} (\mathbf{X}\theta - \mathbf{Y})^T (\mathbf{X}\theta - \mathbf{Y})$$ 

其中，$\mathbf{Y}$ 为观测值向量：
 $$\mathbf{Y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}$$ 

## 4. 求解最优解

为了找到使损失函数最小的参数 $\theta$，需要对损失函数求导数并设其为零。损失函数对 $\theta$ 的导数为：
 $$\frac{\partial J(\theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \left( \frac{1}{2m} (\mathbf{X}\theta - \mathbf{Y})^T (\mathbf{X}\theta - \mathbf{Y}) \right)$$ 

通过链式法则和矩阵导数的性质，可以得到：
 $$\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m} \mathbf{X}^T (\mathbf{X}\theta - \mathbf{Y})$$ 

将导数设为零：
 $$\mathbf{X}^T (\mathbf{X}\theta - \mathbf{Y}) = 0$$ 

## 5. 最优解的求解

解上面的方程，可以得到：
 $$\mathbf{X}^T\mathbf{X}\theta = \mathbf{X}^T\mathbf{Y}$$ 
假设 $\mathbf{X}^T\mathbf{X}$  是可逆的，可以求得 $\theta$：
 $$\theta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$

这就是最小二乘法求解参数的公式。

# 6. 例子

我们希望找出一条最匹配给定数据点的直线  $y = \beta_2 x + \beta_1$ 。这里是最小二乘法的基本步骤：

1. **定义数据点**：
   -  $(1, 6)$
   -  $(2, 5)$
   -  $(3, 7)$
   -  $(4, 10)$

2. **构建矩阵**：
   - 矩阵  $X$  是包含每个  $x$  值和一个常数项的矩阵。
   - 矩阵  $Y$  是包含每个  $y$  值的向量。

3. **求解最优参数**：
   - 使用公式  $\theta = (X^T X)^{-1} X^T Y$  计算最优解。

***

### 第一步：定义数据点

我们先定义 \( X \) 矩阵和 \( Y \) 向量：

 $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}, \quad Y = \begin{bmatrix} 6 \\ 5 \\ 7 \\ 10 \end{bmatrix}$ 

### 第二步：计算最优参数

使用代码来计算最优参数  $\beta_1$  和  $\beta_2$ 。

```python
import numpy as np

# 定义矩阵 X 和向量 Y
X = np.array([
    [1, 1],
    [1, 2],
    [1, 3],
    [1, 4]
])
Y = np.array([6, 5, 7, 10])

# 计算最优参数 θ
theta = np.linalg.inv(X.T @ X) @ X.T @ Y

# 打印结果
theta
```

## 第三步：验证结果

矩阵求解过程如下：

![](https://img.tucang.cc/api/image/show/a4ad40fb54be3b73c9d4a2a88efaf6e0)

拟合直线如下：

<iframe src="https://www.geogebra.org/classic/sujkvht7?embed" width="800" height="600" allowfullscreen style="border: 1px solid #e4e4e4;border-radius: 4px;" frameborder="0"></iframe>
