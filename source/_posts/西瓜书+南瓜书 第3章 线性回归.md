---
title: 西瓜书+南瓜书 第3章 线性回归
tags: 机器学习
swiper_index: 14
categories: 机器学习
description: 线性回归相关知识
cover: 'https://img.tucang.cc/api/image/show/c4c129b1c4f8f3ad5b0e7bccc6960175'
date: '2024-06-22 23:05:08'
abbrlink: 1a1b3c2c
---

# 线性回归

**线性回归**（linear regression）是一种用于预测回归问题的算法.

## 一、线性回归模型

**线性回归**是对“目标变量随着某个特征变量的增大而增大（或者减小）”这种关联性建模的方法。其假设目标变量 $y$ 是自变量 $\mathbf{x}$ 的线性函数。可以分为一元线性回归和多元线性回归。

![](https://img.tucang.cc/api/image/show/ca5dab2a81bb8808115745f6cfb52afe)

### 1. 一元线性回归

一元线性回归是最简单的线性回归模型，它只有一个自变量 $x$。其数学表达式为：

$$
y = wx + b
$$

其中，$w$ 是斜率（或叫权重），$b$ 是截距。

### 2. 多元线性回归

多元线性回归扩展了一元线性回归，它包含多个自变量。其数学表达式为：

$$
y = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b
$$

用向量表示为：

$$
y = \mathbf{w}^T \mathbf{x} + b
$$

其中，$\mathbf{w} = (w_1, w_2, \ldots, w_d)$，$\mathbf{x} = (x_1, x_2, \ldots, x_d)$。

## 二、损失函数和最优化算法
![](https://img.tucang.cc/api/image/show/5108834d363d956c0c05bc464efc5e26)

在线性回归中，常用的损失函数是均方误差（Mean Squared Error, MSE）。均方误差衡量了预测值与实际值之间的差异，其公式为：

$$
E(\mathbf{w}, b) = \sum_{i=1}^m (y_i - (\mathbf{w}^T \mathbf{x}_i + b))^2
$$

最优化算法的目标是找到使损失函数最小的参数 $\mathbf{w}$ 和 $b$。基于均方误差最小化来进行模拟求解的方法称为 **“最小二乘法”**。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

## 三、最小二乘法

最小二乘法通过最小化预测值与实际值之间的平方误差来求解参数。下面详细介绍一元线性回归和多元线性回归中最小二乘法的计算。

### 1. 一元线性回归的最小二乘法

对于一元线性回归，损失函数为：

$$
E_{(w, b)} = \sum_{i=1}^{m} (y_i - (wx_i + b))^2
$$

对 $w$ 和 $b$ 求导得到：

$$
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial w} &= 2 \left( w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i - b)x_i \right), \\
\frac{\partial E_{(w, b)}}{\partial b} &= 2 \left( mb - \sum_{i=1}^m (y_i - wx_i) \right),
\end{aligned}
$$

上式推导过程如下：

$$
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial w} &= \frac{\partial}{\partial w} \left[ \sum_{i=1}^m (y_i - wx_i - b)^2 \right] \\
&= \sum_{i=1}^m \frac{\partial}{\partial w} \left[ (y_i - wx_i - b)^2 \right] \\
&= \sum_{i=1}^m \left[ 2(y_i - wx_i - b)(-x_i) \right] \\
&= 2 \sum_{i=1}^m \left[ (wx_i^2 - y_i x_i + bx_i) \right] \\
&= 2 \left( w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m y_i x_i + b \sum_{i=1}^m x_i \right) \\
&= 2 \left( w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i - b) x_i \right)
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial b} &= \frac{\partial}{\partial b} \left[ \sum_{i=1}^m (y_i - wx_i - b)^2 \right] \\
&= \sum_{i=1}^m \frac{\partial}{\partial b} \left[ (y_i - wx_i - b)^2 \right] \\
&= \sum_{i=1}^m \left[ 2(y_i - wx_i - b)(-1) \right] \\
&= 2 \sum_{i=1}^m \left[ b - y_i + wx_i \right] \\
&= 2 \left[ \sum_{i=1}^m b - \sum_{i=1}^m y_i + \sum_{i=1}^m wx_i \right] \\
&= 2 \left( mb - \sum_{i=1}^m (y_i - wx_i) \right)
\end{aligned}
$$

令上式为零可得到 $w$ 和 $b$ 最优解的闭式解：

$$
w = \frac{\sum_{i=1}^m y_i (x_i - \overline{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m} \left( \sum_{i=1}^m x_i \right)^2}
$$

$$
b = \frac{1}{m} \sum_{i=1}^m (y_i - wx_i)
$$

### 2. 多元线性回归的最小二乘法

我们把 $w$ 和 $b$ 吸收入向量形式 $\hat{\mathbf{w}} = (\mathbf{w}; b)$：

$$
\mathbf{X} = \begin{pmatrix}
x_{11} & x_{12} & \ldots & x_{1d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m1} & x_{m2} & \ldots & x_{md} & 1
\end{pmatrix}
= \begin{pmatrix}
\mathbf{x}_1^\mathrm{T} & 1 \\
\mathbf{x}_2^\mathrm{T} & 1 \\
\vdots & \vdots \\
\mathbf{x}_m^\mathrm{T} & 1
\end{pmatrix}
$$

$$
\hat{\mathbf{w}}^* = \arg\min_{\hat{\mathbf{w}}} \left( \mathbf{y} - \mathbf{X} \hat{\mathbf{w}} \right)^\mathrm{T} \left( \mathbf{y} - \mathbf{X} \hat{\mathbf{w}} \right)
$$

对于多元线性回归，损失函数为：

$$
E_{\hat{\mathbf{w}}} = (\mathbf{y} - \mathbf{X} \hat{\mathbf{w}})^\mathrm{T} \left( \mathbf{y} - \mathbf{X} \hat{\mathbf{w}} \right)
$$

对 $\hat{\mathbf{w}}$ 求导得到：

$$
\frac{\partial E_{\hat{\mathbf{w}}}}{\partial \hat{\mathbf{w}}} = 2 \mathbf{X}^\mathrm{T} \left( \mathbf{X} \hat{\mathbf{w}} - \mathbf{y} \right)
$$

令上式为零可得：

$$
\hat{\mathbf{w}}^* = \left( \mathbf{X}^\mathrm{T} \mathbf{X} \right)^{-1} \mathbf{X}^\mathrm{T} \mathbf{y}
$$


## 四、代码实现
```python
import numpy as np

# 构建数据集
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])  # 输入变量
y = np.array([1, 2, 2, 3])  # 目标变量

# 添加偏置项
X_b = np.c_[np.ones((X.shape[0], 1)), X]

# 最小二乘法求解
beta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# 预测
X_new = np.array([[0, 2], [2, 5]])
X_new_b = np.c_[np.ones((X_new.shape[0], 1)), X_new]
y_predict = X_new_b.dot(beta_best)

print("预测结果:", y_predict)
```
## 五、总结
线性回归通过最小二乘法来最小化预测值和实际值之间的误差，从而得到模型参数。一元线性回归适用于单一特征，多元线性回归适用于多特征。

**参考文献：**
> - 周志华. 机器学习[M]. 北京：清华大学出版社，2016.
> - 谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M]. 人民邮电出版社，2023
> - 视频资料：[吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导](https://www.bilibili.com/video/BV1Mh411e7VU?p=1&vd_source=1a092a7225edef98ec15a416dcdedb80)